# Coral-DeepLab (ì„¼ì„œ ìœµí•©)

<img src="https://img.shields.io/badge/TensorFlow-2.x-orange"/>
<img src="https://img.shields.io/badge/Edge%20TPU-ready-brightgreen"/>

ì„¼ì„œ ì •ë³´(IMUÂ·ê¸°ìƒÂ·GPS ë“±)ì™€ RGB ì´ë¯¸ì§€ë¥¼ ë™ì‹œì— ì´ìš©í•˜ëŠ” ëª¨ë°”ì¼ ì„¸ê·¸ë©˜í…Œì´ì…˜ ë„¤íŠ¸ì›Œí¬ **Coral-DeepLab**ì˜ TensorFlow/Keras êµ¬í˜„ì…ë‹ˆë‹¤.  
Google Coral Edge-TPU ë° Raspberry Pi(XNNPACK)ì—ì„œ ì‹¤ì‹œê°„ ì¶”ë¡ ì´ ê°€ëŠ¥í•˜ë„ë¡ ëª¨ë¸ êµ¬ì¡°ë¥¼ ê²½ëŸ‰í™”í•˜ê³ , ì™„ì „ ì •ìˆ˜(INT8) ì–‘ìí™”ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.

---
## ëª©ì°¨
1. [ì£¼ìš” íŠ¹ì§•](#ì£¼ìš”-íŠ¹ì§•)  
2. [ë””ë ‰í„°ë¦¬ êµ¬ì¡°](#ë””ë ‰í„°ë¦¬-êµ¬ì¡°)  
3. [ì„¤ì¹˜](#ì„¤ì¹˜)  
4. [ë°ì´í„° ì¤€ë¹„](#ë°ì´í„°-ì¤€ë¹„)  
5. [í•™ìŠµ](#í•™ìŠµ)  
6. [ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸](#ì „ì²˜ë¦¬-íŒŒì´í”„ë¼ì¸)  
7. [ì²´í¬í¬ì¸íŠ¸ ì´ì–´-í•™ìŠµ](#ì²´í¬í¬ì¸íŠ¸-ì´ì–´-í•™ìŠµ)  
8. [ëª¨ë¸ ì–‘ìí™” & ì»´íŒŒì¼](#ëª¨ë¸-ì–‘ìí™”--ì»´íŒŒì¼)  
9. [ì¶”ë¡ ](#ì¶”ë¡ )  
10. [ì°¸ê³  ë¬¸í—Œ](#ì°¸ê³ -ë¬¸í—Œ)  

---
## ì£¼ìš” íŠ¹ì§•
* **DeepLab V3 / V3+** ë°±ë³¸: MobileNet V2 (Î± ê°€ì¤‘ì¹˜ ì¡°ì ˆ ì§€ì›)
* **SensorVisionFusion**: 6-ì°¨ì› ì„¼ì„œ ë²¡í„°ë¥¼ 1Ã—1Ã—C ì„ë² ë”© í›„ feature mapì— ê°€ì¤‘ í•©ì‚°
* **CBAM**(ì„ íƒ): Channel & Spatial Attention ëª¨ë“ˆ
* **ì™„ì „ ì •ìˆ˜ ì–‘ìí™”** â†’ Edge-TPU / Lite RT(XNNPACK) í˜¸í™˜
* **DataLoader**: COCO-style JSON + `sensor_info` í•„ë“œë¥¼ ì§€ì›í•˜ëŠ” `tf.data` íŒŒì´í”„ë¼ì¸

---
## ë””ë ‰í„°ë¦¬ êµ¬ì¡°
```
â”œâ”€â”€ coral_deeplab/          # ë¼ì´ë¸ŒëŸ¬ë¦¬ ì†ŒìŠ¤
â”‚   â”œâ”€â”€ _blocks.py          # ASPP Â· Decoder Â· Fusion ë˜í¼
â”‚   â”œâ”€â”€ fusion.py           # SensorVisionFusion ë ˆì´ì–´
â”‚   â”œâ”€â”€ applications.py     # CoralDeepLabV3 / V3Plus ë¹Œë”
â”‚   â””â”€â”€ utils/              # ë°ì´í„°ì…‹ Â· ë³€í™˜ í—¬í¼
â”œâ”€â”€ config/                 # dataclass ê¸°ë°˜ ì„¤ì • ëª¨ë“ˆ
â”œâ”€â”€ data/                   # COCO JSON & ì´ë¯¸ì§€ (ì‚¬ìš©ì ì¤€ë¹„)
â”œâ”€â”€ example/                # ìƒ˜í”Œ ì´ë¯¸ì§€/ì„¼ì„œ JSON
â”œâ”€â”€ checkpoints_tf/         # Keras ê°€ì¤‘ì¹˜ (*.keras)
â”œâ”€â”€ compiler/               # Edge-TPU compiler wrapper
â”œâ”€â”€ converter_*.py          # TFLite ë³€í™˜ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ inference*.py           # ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸ (Keras / TFLite)
â””â”€â”€ main.py                 # í•™ìŠµ ì§„ì…ì 
```

---
## ì„¤ì¹˜
```bash
python -m venv .venv && source .venv/bin/activate
pip install --upgrade pip
pip install -r requirements.txt            # TensorFlow 2.x, numpy â‰¤1.24 ë“±
# OpenCV(í—¤ë“œë¦¬ìŠ¤) ì¶”ê°€
pip install opencv-python-headless
```
Edge-TPU ì»´íŒŒì¼ì„ ìœ„í•´ì„œëŠ” Coral SDK(`edgetpu_compiler v14+`)ê°€ í•„ìš”í•©ë‹ˆë‹¤.  
`compiler/compiler.sh` ìŠ¤í¬ë¦½íŠ¸ë¡œ ìë™ ì„¤ì¹˜ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.

---
## ë°ì´í„° ì¤€ë¹„
1. **ì´ë¯¸ì§€**: `data/images/` í´ë”ì— RGB íŒŒì¼ ë°°ì¹˜  
2. **ì£¼ì„(JSON)**: COCO í¬ë§·
```jsonc
{
  "images": [
    { "id": 1, "file_name": "xxx.jpg", "sensor_info": { /* ì•„ë˜ ì˜ˆì‹œ */ } },
    ...
  ],
  "annotations": [ { "image_id": 1, "category_id": 3, "segmentation": [...] }, ... ],
  "categories": [ {"id":1,"name":"car"}, ... ]
}
```
3. **sensor_info í•„ë“œ** (í‚¤Â·ìˆœì„œ ê³ ì •)
```jsonc
{
  "objectTemp": 24.8,    // Â°C  (-100~100)
  "humi": 79.3,          // %   (0~100)
  "pressure": 1013.1,    // hPa (950~1050)
  "latitude": 37.42,     // Â°   (-90~90)
  "longitude": 126.89,   // Â°   (-180~180)
  "height": 4.7          // m   (0~1000)
}
```
ë°°ê²½ ë¼ë²¨ì€ **ID 0** ìœ¼ë¡œ ì•”ë¬µì ìœ¼ë¡œ í• ë‹¹ë©ë‹ˆë‹¤.

---
## í•™ìŠµ
```bash
python main.py \
  --train_annotations data/COCO/train.json \
  --val_annotations   data/COCO/valid.json \
  --train_images      data/images \
  --val_images        data/images \
  --model deeplabv3plus --batch_size 64 --epochs 200
```
ì˜µì…˜
* `--model` : `deeplabv3` | `deeplabv3plus`
* `--lr`    : ì´ˆê¸° í•™ìŠµë¥  (ê¸°ë³¸ 1e-4)
* `--resume`: `checkpoints_tf/epoch_XXX.keras` ì´ì–´-í•™ìŠµ

---
## ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
| í•­ëª© | dtype | ë²”ìœ„ | ë¹„ê³  |
|------|-------|------|------|
| ì´ë¯¸ì§€ | float32 | 0 â€“ 1 | `OpenCV BGR â†’ RGB /255.0` |
| ì„¼ì„œ   | float32 | 0 â€“ 255 | `mins/maxs` í…Œì´ë¸”ë¡œ í´ë¦½ & ìŠ¤ì¼€ì¼ |

ëª¨ë¸ ì…ë ¥: `(image, sensors)`  ğŸ‘ˆ ë‘ í…ì„œë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ì „ë‹¬í•©ë‹ˆë‹¤.

### ì„¼ì„œ ì „ì²˜ë¦¬ í•¨ìˆ˜ (ë§¤ìš° ì¤‘ìš”) `_sensor_to_vec`

ë°ì´í„° ë¡œë”ì™€ ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸ì— ê³µí†µìœ¼ë¡œ ì“°ì´ëŠ” í•¨ìˆ˜ë¡œ, ì„¼ì„œ JSON(ë˜ëŠ” dict) â†’ `float32 (6,)` ë²¡í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

1. **í•„ë“œ ì¶”ì¶œ**  
   `objectTemp`, `humi`, `pressure`, `latitude`, `longitude`, `height` ì—¬ì„¯ í‚¤ë¥¼ ê³ ì • ìˆœì„œë¡œ ì½ì–´ì˜µë‹ˆë‹¤.  
   ëˆ„ë½ëœ ê°’ì€ 0.0 ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.

2. **ë²”ìœ„ í´ë¦¬í•‘ & 0-255 ì •ê·œí™”**  
   | í•­ëª© | min | max |
   |------|-----|-----|
   | objectTemp | â€“100 | 100 |
   | humi       |   0  | 100 |
   | pressure   | 950  | 1050|
   | latitude   | â€“90  | 90  |
   | longitude  | â€“180 | 180 |
   | height     | 0    | 1000|

   ê°’ì´ ìœ„ ë²”ìœ„ë¥¼ ë„˜ìœ¼ë©´ `np.clip`ìœ¼ë¡œ ì˜ë¼ë‚¸ ë’¤  
   `(value-min) Ã— 255 / (max-min)` ì‹ì„ ì‚¬ìš©í•´ **0~255 float32** ê°’ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

3. **ë°°ì¹˜ ì°¨ì› ì¶”ê°€**  
   í•™ìŠµ ì‹œì—ëŠ” `(B, 6)`, ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸ì—ì„œëŠ” `(1, 6)` í˜•íƒœë¡œ ë˜í•‘í•´ ëª¨ë¸ì— ì „ë‹¬í•©ë‹ˆë‹¤.

ì´ ê³¼ì •ì„ í†µí•´ í•™ìŠµÂ·ì–‘ìí™”Â·ì¶”ë¡  ëª¨ë“  ë‹¨ê³„ì—ì„œ ì„¼ì„œ ì…ë ¥ ë¶„í¬ê°€ ì™„ì „íˆ ì¼ì¹˜í•˜ê²Œ ë©ë‹ˆë‹¤.

---
## ì²´í¬í¬ì¸íŠ¸ ì´ì–´-í•™ìŠµ
```bash
python main.py --batch_size 64 --epochs 500 \
               --resume checkpoints_tf/epoch_144.keras
```
`epoch_144.keras` ê¹Œì§€ í•™ìŠµëœ ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ **epoch 145** ë¶€í„° ì´ì–´ì„œ í•™ìŠµí•©ë‹ˆë‹¤.

---
## ëª¨ë¸ ì–‘ìí™” & ì»´íŒŒì¼
### Raspberry Pi (XNNPACK)
```bash
python converter_rpi_sensor.py            # seg_model_int8.tflite ìƒì„±
```
### Edge-TPU
```bash
bash compiler/compiler.sh seg_model_int8.tflite
# â†’ seg_model_int8_edgetpu.tflite
```

### ëŒ€í‘œ ë°ì´í„°ì…‹ ìŠ¤ì¼€ì¼ ì£¼ì˜
> **ëŒ€í‘œ ë°ì´í„°ì…‹ ìŠ¤ì¼€ì¼ ì£¼ì˜**  
>   â€¢ **ì´ë¯¸ì§€**: `rand()  â†’  0 ~ 1` *(ë³€í™˜ê¸°ì—ì„œ scaleâ‰ˆ1/255 ì¶”ì •)*  
>   â€¢ **ì„¼ì„œ**  : `rand()*255 â†’ 0 ~ 255` *(scaleâ‰ˆ1 ì¶”ì •)*  
>   ì´ë ‡ê²Œ êµ¬ì„±í•´ì•¼ ì™„ì „ ì •ìˆ˜ ì–‘ìí™” ëª¨ë¸ì´ ì´ë¯¸ì§€/ì„¼ì„œ ê°ê°ì— ë§ëŠ” scale ê°’ì„ ì €ì¥í•©ë‹ˆë‹¤. 0â€†~â€†255 `uint8` ì´ë¯¸ì§€ë¥¼ ê·¸ëŒ€ë¡œ ë„£ì–´ë„ `(valâˆ’0)Ã—1/255` ë¡œ 0â€†~â€†1 ë¡œ ë³µì›ë˜ì–´ Keras ê²°ê³¼ì™€ ê±°ì˜ ë™ì¼í•´ì§‘ë‹ˆë‹¤.

### CPU ê¸°ë³¸ ì‹¤í–‰ ë° TPU ì„ íƒì  ì‚¬ìš©
* ë³€í™˜Â·ì–‘ìí™” ê³¼ì •ì€ **CPU** ë§Œìœ¼ë¡œ ìˆ˜í–‰ë©ë‹ˆë‹¤. CUDA / GPUê°€ ì—†ì–´ë„ ë¬¸ì œ ì—†ìŠµë‹ˆë‹¤.
* ì¶”ë¡  ì—­ì‹œ ê¸°ë³¸ì ìœ¼ë¡œ **CPU(XNNPACK delegate)** ê²½ë¡œë¥¼ ì‚¬ìš©í•˜ë©°, `--delegate edgetpu` í”Œë˜ê·¸ë¥¼ ì¤˜ì•¼ë§Œ Edge-TPU ê°€ì†ì´ í™œì„±í™”ë©ë‹ˆë‹¤.

---
## ì¶”ë¡ 
### Keras ì²´í¬í¬ì¸íŠ¸
```bash
python inference_with_keras.py \
  --input data/images/example.jpg \
  --sensor_json data/images/example.json \
  --ckpt checkpoints_tf/epoch_187.keras \
  --save_mask --output_dir results
```
### TFLite ëª¨ë¸ (CPU/XNNPACK ë˜ëŠ” Edge-TPU)
```bash
python inference.py \
  --input data/images/example.jpg \
  --seg_model seg_model_int8_edgetpu.tflite \
  --delegate edgetpu --save_mask
```

---
## í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ì„±ëŠ¥

ë‹¤ìŒ í‘œëŠ” *test* split ì „ì²´ì— ëŒ€í•´ `test_with_keras.py` ë° `test_with_int8.py` ë¡œ ì¸¡ì •í•œ ì£¼ìš” ì§€í‘œì…ë‹ˆë‹¤.

| ëª¨ë¸ | Pixel Accuracy | Mean IoU | Mean Dice | Frequency-Weighted IoU |
|------|---------------|----------|-----------|------------------------|
| **Keras (float32)** | **0.9302** | **0.7922** | **0.8809** | **0.8760** |
| **TFLite INT8** | 0.9271 | 0.7843 | 0.8757 | 0.8712 |

> *ì°¸ê³ *: Keras ëª¨ë¸ ëŒ€ë¹„ INT8 TFLite ëª¨ë¸ì€ ì–‘ìí™” ì†ì‹¤ë¡œ ì¸í•´ mIoU ì•½ **0.8pt** ì •ë„, Pixel Accuracy ì•½ **0.3pt** ê°ì†Œí–ˆì§€ë§Œ, ì „ë°˜ì ìœ¼ë¡œ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ìœ ì§€í•©ë‹ˆë‹¤. Edge-TPU delegate ì‚¬ìš© ì‹œ ìµœëŒ€ **x10** ë°°ì˜ ì¶”ë¡  ì†ë„ í–¥ìƒì„ ê¸°ëŒ€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---
## ì°¸ê³  ë¬¸í—Œ
* Chen et al., *Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation*, 2018  
* Woo et al., *CBAM: Convolutional Block Attention Module*, 2018  
* Google Coral, [Semantic Segmentation with Edge-TPU](https://coral.ai/models/)